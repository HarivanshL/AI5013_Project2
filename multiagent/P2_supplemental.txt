Please Enter your team's full names and your answers to the questions marked by QS questions here!

Team:
Dohyeong Park
Harivansh Luchmun

Q1.1:
The ReflexAgent in multiAgents.py selects actions based on an evaluation function 
that considers the successor game state after taking an action. Initially, the agent 
only used the game score, but I improved it by using food distance, number of remain 
food, ghost distance, and power capsule distance to make more strategic decisions. 
The enhanced agent prioritizes eating nearby food, avoids dangerous ghosts, and 
actively seeks power capsules when necessary.

Q1.2:
My value function estimates how good a game state is. The return value is based on 
the score. I subtract the minimum food distance from the score to make pacman move
to food which makes it keeps positive score. Also, I subtract the number of remain 
foods from the score to pacman actively eat the last foods because I saw that pacman 
takes so long time to eat last one or two foods. I add the minimum ghost distance to 
the score. If ghost is dangerous add small value and if ghost is scared add bigger value.
It will make pacman to avoid the dangereous ghosts to keep positive score Additionally, 
If dangereous ghost is too close, I give penalty. I also subtract the minimum capsule 
distance from the score to make the pacman move to capsule actively to keep positive 
score. This makes sense because pacman needs to eat all the food while avoiding ghosts, 
and this evaluation helps the pacman moves smarter way to win the game.


Q2.1: 
This algorithm creates a minimax agent that evaluates the actions of both pacman and 
the ghosts. They take turns choosing and action. Pacman tries to actively maximize 
his score while the ghosts try to minimize his score. Essentially the algorithm works 
by recursively, by calling the next agent or agents. Either it is the ghosts or pacman.
This creates a tree and the ghosts try to pick the minimum value at each of their 
levels while pacman chooses the best value at each of its levels. The algorithm stops 
when the depth of the tree is 0 or pacanl loses, meaning the game has been solved. We think the 
algorithm is working because it actively avoids ghosts and pacman also loses the game 
in one of the autograder instances.






Q3.1:
The values are identical because the alpha beta pruning only does not 
explore nodes it thinks are unnecessary. So originally it would result 
in the same tree, but with pruning some of the branches are cut off and 
the resultin best actions are still the same. It just saves more time.

Q3.2
We break a tie by still exploring that branch of tree. We only prune if it 
the value is strictly greater than beta for pacmans turn, or strictly less 
than alpha for the ghosts turn


Q4.1:
The expectimax works similar to the minimax algorithm, but it factors in the 
possibility that the ghosts will not make the most optimal move. It works recursively 
and pacman gets a turn then the ghosts. The ghosts try to minimize pacmans actions 
while pacman tries to maximize. The game ends when the depth of the tree is 0 or pacman loses. 
In this algorithm, at each level of ghosts decision, the min agent takes the average of 
all future actions from the node and computes that as that nodes value. This ensures a 
expectimax algorithm.

Q5.1:
This evaluation function is similar to the last evaluation function. It still factors 
in when the ghosts are scared or not scared, the distance to capsules, ghosts, and food. 
It also factors in how much food and capsules are left. This function is better because 
it places a larger penalty on ghosts that are not scared and close. This function also 
assigns 0 to one of the distances for the ghosts based on if they are scared or not. This 
helps us to not factor it in picking the best path.



